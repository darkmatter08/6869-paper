\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{4321}
\begin{document}

%%%%%%%%% TITLE
\title{Visualizing sub-networks in deep convolutional neural networks}

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
   Deep convolutional neural networks have recently demonstrated great success in computer vision problems. While there have been several efforts to understand how how these networks work, we are still far from truth. Moreover, most of the expoloratory work done has been in the domain of understanding object and scene detectors. Our motivation here is two fold - expanding the domain of standard network visualization techniques to the domain of text recognition, and most importantly, suggesting new general purpose visualization pipelines which rely on identifying subnetworks which may be performing a high level function, as opposed to the standard per unit visualization. Such subnetwork identification methods may even be useful in reducing network complexity when invariants of the system are known, and in facilitating transfer learning across different domains of computer vision. 
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
Convolutional neural networks have unprecedented success in various computer vision tasks in the recent past.  With applications ranging from object detection \cite{}, segmentation \cite{}, text recognition \cite{}, CNN's have quickly become a staple in the computer vision community. Their success can be partially attributed to the networks ability to learn invariants in the high dimensional space \cite{}. While the idea was first introduced long back by LeCun et al in early 1990s, several factors have controbuted to their unprecedented success - (i) Availability of much larger training sets with millions of labelled samples, (ii) Powerful GPU's and more computational horsepower making it possible to train networks larger than ever, and (iii) better training strategies for regularization using Dropout \cite{} and batch normalization \cite{}. \cite{Fergus14} 

Despite their successes, CNNs still are not much more than high performing black boxes, with limited understanding about what they have learned, and how they handle different kinds of input. Without a firm understanding of what the CNN is doing, it becomes hard to improve and optimize network typologies and techniques without trial and error. While there are a number of papers working on visualizing the internal representation of hidden layers, most of them have focussed on per-unit visualizations \cite{torralba,mahendran,yosinski etcetcetc}.Moreover, most of this work has been done in the domain of object/scene detection. We take this one step further by extending these existing techniques to the domain of text recognition, which has seen significant success with CNN's \cite{oxford papers}. As text is inherently synthetic data, and because we work with a synthetic dataset, it gives us the power to address aspects which may be missing in object/scene datasets. In particular, transformations such as scaling, rotation, and translation have not been visualized in networks before. 

Further, we believe that keeping in sync with the idea that a network involves computations chained together it makes more sense to visualize a chain of operations as opposed to a single node. To address this, our work attempts to  explore different visualization technques which aim to identify a sub-network, or a path of information flow in the network which may be related closely with either (i) A particular kind of input image, or (ii) A particular kind of output class. We propose a method akin to how a neurologist would map out the brain by mapping out regions of activity what performing specific tasks.\cite{} We experimentally demonstrate the our visualization techniques for transformations by using a network trained to read text in the wild. 

Identifying sub-networks can prove to be extremely useful at different steps in CNN training and deployment. It can help decide leaner network architecture depending upon known invariants in the training dataset, and prune existing networks without reducing accuracy over certain datasets. Furthermore, it can, in principle, facilitate a plug-and-play use of sub-networks for transfer learning to new domains akin to the fine-tuning approach which is gaining popularity in the computer vision community. \cite{}

\subsection{Main Contributions}

\begin{enumerate}
\item A visualization technique to determine activity in a network.

\item A method of reducing network complexity while retaining accuracy when assumptions on the data can be made.

\item An efficient system to create multiple specialized networks out a larger pretrained network.

\item A demonstration of detecting an removing transformation specific units in a network.
\end{enumerate}

\subsection{Main Limitations}
Our visualization technique can be useful for visualizing the network if the units of interest exist in the convolutional layers. If instead they exist in the fully connected layers, not much useful information can be extracted. In addition it is difficult to apply isolated pruning without effecting the entire network.

Another limitation is that pruning the networks graphs may not have a computational boost on hardware that are designed to work with large amount of vectors in parallel, such as GPUs. However, as machine learning becomes more prevalent, applications that utilize lower powered simpler devices will increase and benefit from the computation reduction.

\section{Related Work}
\subsection{CNN Visualization}


Torabla

\subsection{Text Recognition}

Oxford

\subsection{Transfer learning}

maybe? if we want to talk about tuning our network for different text tasks

\section{Datasets and Model}
\subsection{DATASETS}
Talk about MJSynth and Synthetic dataset we generated. 

\subsection{Synthetic text generation pipeline}

\subsection{NIPS15 model}
Describe charnet and oxford pipeline

\subsection{Model Validation}
Give numbers on model validation for MatConvnet and Caffe implementation.

\subsection{Dropping Neurons}
How we choose what and when to drop neurons

\section{Visualization Pipelines}

\subsection{Deep Visualization Toolbox by yosinski et al}

\subsection{Visualizing filter activations}

\subsection{Network architecture invariants - Top Weights Method}

\subsection{Location encoding in networks - Patch Method}

\subsection{Subnetwork Identification from Activations}


\section{Results}

\subsection{Detecting Transformation Specific Units}
Show that we can detect transformation units.

\begin{figure}
%\includegraphics[width=\columnwidth]{}
\caption{Comparison of activations for different transformations}
\label{fig:comp}
\end{figure}

\subsection{Dropping Transformation Specific Units}
Show that we retain (or don't) accuracy if we drop units

\begin{figure}
%\includegraphics[width=\columnwidth]{}
\caption{Plot of accuracy decrease as neurons are removed}
\label{fig:comp}
\end{figure}

%-------------------------------------------------------------------------
\section{Discussion}

\subsection{Better understanding of Deep CNNS}
Talk about why this helps us understand what Deep CNNs are doing

%-------------------------------------------------------------------------
\subsection{Transfer Learning}

Talk about how this can better inform transfer learning because we know what is doing what

%-------------------------------------------------------------------------
\subsection{Reducing Computational Complexity}
Talk about why this pruning process may be useful to improve run times on mobile devices (or non GPU devices).

%-------------------------------------------------------------------------
\subsection{Conclusion}


%------------------------------------------------------------------------

{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}
