\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{4321}
\begin{document}

%%%%%%%%% TITLE
\title{Detecting Transformation Specific Units in Deep Text CNNs}

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
   Isolate neurons for different transforms.
   Drop units and maintain performance
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

Why understanding CNNs is important
Why understanding CNNS is hard
Why we approach transformation units
How we approach the problem


\subsection{Main Contributions}

List of contributions
-Isolate units
-Drop units and maintain accuracy
-Manipulate network without retraining

\subsection{Main Limitations}

?Maybe don't need

Our analysis is specific to one network

\section{Related Work}


\subsection{CNN Visualization}

Torabla

\subsection{Text Recognition}

Oxford

\subsection{Transfer learning}

maybe? if we want to talk about tuning our network for different text tasks

\section{Experimental Pipeline}
\subsection{Generating Test Sets}
How we do it. Why we vary things

\begin{figure}
%\includegraphics[width=\columnwidth]{}
\caption{example of different text renders}
\label{fig:text}
\end{figure}

\subsection{Visualizing Activations}
How we visualize activations
\begin{figure}
%\includegraphics[width=\columnwidth]{}
\caption{example of visualization}
\label{fig:vis}
\end{figure}

\subsection{Dropping Neurons}
How we choose what and when to drop neurons

\section{Results}

\subsection{Detecting Transformation Specific Units}
Show that we can detect transformation units.

\begin{figure}
%\includegraphics[width=\columnwidth]{}
\caption{Comparison of activations for different transformations}
\label{fig:comp}
\end{figure}

\subsection{Dropping Transformation Specific Units}
Show that we retain (or don't) accuracy if we drop units

\begin{figure}
%\includegraphics[width=\columnwidth]{}
\caption{Plot of accuracy decrease as neurons are removed}
\label{fig:comp}
\end{figure}

%-------------------------------------------------------------------------
\section{Discussion}

\subsection{Better understanding of Deep CNNS}
Talk about why this helps us understand what Deep CNNs are doing

%-------------------------------------------------------------------------
\subsection{Transfer Learning}

Talk about how this can better inform transfer learning because we know what is doing what

%-------------------------------------------------------------------------
\subsection{Reducing Computational Complexity}
Talk about why this pruning process may be useful to improve run times on mobile devices (or non GPU devices).

%-------------------------------------------------------------------------
\subsection{Conclusion}


%------------------------------------------------------------------------

{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}
